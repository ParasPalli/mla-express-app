{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punjab MLA Srapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from details screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "find() got multiple values for argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-8a4627dc6495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Extract Mobile Number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mphone_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Mobile Number'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mcontact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphone_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mphone_tag\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: find() got multiple values for argument 'name'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium with Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run headless (without GUI)\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome('../chrome_driver/chromedriver.exe')\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.punjabassembly.nic.in/Members/MembersProfile?MemberID=4&AssemblyID=16\"\n",
    "\n",
    "# Visit the page\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and click the 'Proceed' button (example)\n",
    "driver.find_element(By.ID, \"details-button\").click()  # Adjust based on actual element ID\n",
    "\n",
    "# Get the page source after interaction\n",
    "driver.find_element(By.ID, 'proceed-link').click()\n",
    "\n",
    "\n",
    "# Get the page source after interaction\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "\n",
    "# Extract the required details\n",
    "# Extract Name\n",
    "name_tag = soup.find(\"span\", string=\"Sh. Aman Arora\")  # The exact name to search for\n",
    "name = name_tag.text.strip() if name_tag else \"Not Found\"\n",
    "\n",
    "# Extract State Name\n",
    "state_tag = soup.find(\"span\", string=\"Punjab\")  # The exact state to search for\n",
    "state = state_tag.text.strip() if state_tag else \"Not Found\"\n",
    "\n",
    "# Extract Constituency Name\n",
    "constituency_tag = soup.find(\"p\", class_=\"text-wrap\")\n",
    "constituency = constituency_tag.text.strip() if constituency_tag else \"\"\n",
    "\n",
    "# Extract Mobile Number\n",
    "phone_tag = soup.find(\"p\", class_=\"text-wrap\").find(\"a\")\n",
    "contact = phone_tag.text.strip() if phone_tag else \"\"\n",
    "\n",
    "# Extract Email\n",
    "email_tag = soup.find(\"p\", class_=\"text-wrap\").find(\"a\")\n",
    "email = email_tag.text.strip() if email_tag else \"\"\n",
    "\n",
    "# Extract Party Name\n",
    "party_tag = soup.find(\"h6\", class_=\"text-wrap\")\n",
    "party_name = party_tag.text.strip() if party_tag else \"Not Found\"\n",
    "\n",
    "# Gender and email are not available on the page based on this example\n",
    "gender = \"\"\n",
    "\n",
    "# Output the extracted details\n",
    "print({\n",
    "    \"Constituency\": constituency,\n",
    "    \"Name\": name,\n",
    "    \"Gender\": gender,\n",
    "    \"Party\": party_name,\n",
    "    \"Contact\": contact,\n",
    "    \"Email\": email,\n",
    "})\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_page_links = []\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_through_details(driver):\n",
    "    \n",
    "    for url in detail_page_links:\n",
    "\n",
    "        driver.implicitly_wait(2)\n",
    "        driver.get('https://www.punjabassembly.nic.in' + url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        name_tag = soup.find(\"span\", string=\"Sh. Aman Arora\")  # The exact name to search for\n",
    "        name = name_tag.text.strip() if name_tag else \"Not Found\"\n",
    "\n",
    "        # Extract State Name\n",
    "        state_tag = soup.find(\"span\", string=\"Punjab\")  # The exact state to search for\n",
    "        state = state_tag.text.strip() if state_tag else \"Not Found\"\n",
    "\n",
    "        # Extract Constituency Name\n",
    "        constituency_tag = soup.find(\"p\", class_=\"text-wrap\")\n",
    "        constituency = constituency_tag.text.strip() if constituency_tag else \"\"\n",
    "\n",
    "        # Extract Mobile Number\n",
    "        if (soup.find(\"p\", class_=\"text-wrap\") != None):\n",
    "            phone_tag = soup.find(\"p\", class_=\"text-wrap\").find(\"a\")\n",
    "            contact = phone_tag.text.strip() if phone_tag else \"\"\n",
    "        else:\n",
    "            contact = \"\"\n",
    "\n",
    "        # Extract Email\n",
    "        if (soup.find(\"p\", class_=\"text-wrap\") != None):\n",
    "            email_tag = soup.find(\"p\", class_=\"text-wrap\").find(\"a\")\n",
    "            email = email_tag.text.strip() if email_tag else \"\"\n",
    "        else:\n",
    "            email = \"\"\n",
    "\n",
    "        # Extract Party Name\n",
    "        party_tag = soup.find(\"h6\", class_=\"text-wrap\")\n",
    "        party_name = party_tag.text.strip() if party_tag else \"Not Found\"\n",
    "\n",
    "        # Gender and email are not available on the page based on this example\n",
    "        gender = \"\"\n",
    "\n",
    "        # Output the extracted details\n",
    "        data.append({\n",
    "            \"Constituency\": constituency,\n",
    "            \"Name\": name,\n",
    "            \"Gender\": gender,\n",
    "            \"Party\": party_name,\n",
    "            \"Contact\": contact,\n",
    "            \"Email\": email,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Links from Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up Selenium with Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run headless (without GUI)\n",
    "driver = webdriver.Chrome('../chrome_driver/chromedriver.exe')\n",
    "\n",
    "for x in range(1, 6):\n",
    "\n",
    "    url = f\"https://www.punjabassembly.nic.in/Members?Page_No={x}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    if x == 1:\n",
    "        # Processing the non-trusted site\n",
    "        driver.find_element(By.ID, \"details-button\").click()\n",
    "        driver.find_element(By.ID, 'proceed-link').click()\n",
    "\n",
    "\n",
    "    # Starting Extraction\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    view_more_tags = soup.find_all(\"a\", href=True, string=\"View More\")\n",
    "    view_more_links = [tag['href'] for tag in view_more_tags]\n",
    "\n",
    "    # Append the Links\n",
    "    detail_page_links.extend(view_more_links)\n",
    "\n",
    "    # Output the extracted links\n",
    "    print(\"View More Links:\")\n",
    "    for link in view_more_links:\n",
    "        print('https://www.punjabassembly.nic.in' + link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to pubjab_mla_contact_details.csv\n"
     ]
    }
   ],
   "source": [
    "# Scrap Detail Pages\n",
    "scrap_through_details(driver)\n",
    "\n",
    "# Quit Driver\n",
    "driver.quit()\n",
    "\n",
    "# Save the Data\n",
    "# Save data to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('pubjab_mla_contact_details.csv', index=False)\n",
    "print(\"Data saved to pubjab_mla_contact_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
